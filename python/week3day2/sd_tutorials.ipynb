{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Transformers & Stable Diffusion\n",
    "> éƒ‘é—®è¿ª\n",
    "> zhengwd23@mails.tsinghua.edu.cn\n",
    "\n",
    "- Huggingface Transformers: https://huggingface.co/\n",
    "\n",
    "    HuggingfaceğŸ¤— (HF) æ˜¯ä¸€ä¸ªæä¾›æ•°æ®ç§‘å­¦ä¸æœºå™¨å­¦ä¹ ç›¸å…³å¼€æºæ¨¡å‹åŠæ•°æ®çš„å¹³å°ã€‚\n",
    "    \n",
    "    é€šè¿‡HFæä¾›çš„`transformers`åº“ï¼Œä½ å¯ä»¥ä»…ä½¿ç”¨è‹¥å¹²è¡Œä»£ç éå¸¸ä¾¿æ·åœ°è°ƒç”¨å¼€æºçš„æ¨¡å‹å®ç°æ¨ç†åŠè®­ç»ƒç­‰è‡ªå®šä¹‰éœ€æ±‚ã€‚\n",
    "- Stable Diffusion: https://github.com/CompVis/stable-diffusion\n",
    "\n",
    "    æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£ç”Ÿæˆæ¨¡å‹ï¼ˆDiffusion Models for Text-to-Imageï¼‰ä¸­å…·æœ‰ä»£è¡¨æ€§çš„å¼€æºå·¥ä½œã€‚\n",
    "    \n",
    "    Stable Diffusion (SD) æ˜¯ç›®å‰æ–‡åˆ°å›¾ç”Ÿæˆç¤¾åŒºä½¿ç”¨çš„ä¸»æµåŸºç¡€æ¨¡å‹ã€‚\n",
    "    \n",
    "    å’Œå®˜æ–¹ä»“åº“ç›¸æ¯”ï¼Œ[webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) è¿™ä¸ªç”±æ°‘é—´å¤§ä½¬åœ¨æ¨¡å‹åŸºç¡€ä¸Šå¼€å‘çš„é›†æˆä»“åº“ä¼¼ä¹æ›´å—å¤§å®¶æ¬¢è¿ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface Transformers\n",
    "\n",
    "> æ›´å¤šå¯å‚ç…§HFå®˜æ–¹tutorialï¼šhttps://huggingface.co/docs/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºäº torch / tensorflow\n",
    "# æœ¬æ–‡æ¡£åŸºäºtorchæ¼”ç¤º\n",
    "# å®‰è£…\n",
    "%pip install transformers datasets\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transformers.pipeline`: ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†çš„API\n",
    "\n",
    "éƒ¨åˆ†ä»»åŠ¡çš„ç”¨æ³•ç¤ºä¾‹å¦‚ä¸‹ï¼š\n",
    "| ä»»åŠ¡ | ç”¨æ³• |\n",
    "| :-- | :-- |\n",
    "| æ–‡æœ¬åˆ†ç±»ï¼ˆText classificationï¼‰      | pipeline(task=â€œsentiment-analysisâ€)   |\n",
    "| æ–‡æœ¬ç”Ÿæˆï¼ˆText generationï¼‰          | pipeline(task=â€œtext-generationâ€)      |\n",
    "| å›¾åƒåˆ†ç±»ï¼ˆImage classificationï¼‰     | pipeline(task=â€œimage-classificationâ€) |\n",
    "| ç›®æ ‡æ£€æµ‹ï¼ˆImage classificationï¼‰     | pipeline(task=â€œimage-segmentationâ€)   |\n",
    "| è§†è§‰é—®ç­”ï¼ˆVisual question answeringï¼‰| pipeline(task=â€œvqaâ€)                  |\n",
    "| å›¾åƒæ ‡æ³¨ï¼ˆImage captioningï¼‰         | pipeline(task=â€œimage-to-textâ€)        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»¥æƒ…æ„Ÿåˆ†ç±»ä¸ºä¾‹\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "results = classifier([\n",
    "    \"We are very happy to introduce python and transformers to you.\",\n",
    "    \"So sad you didn't attend the class.\"\n",
    "])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨pipelineä¸­ä½¿ç”¨éé»˜è®¤çš„å¦å¤–æŒ‡å®šçš„moduleï¼ˆæ¯”å¦‚modelå’Œtokenizerï¼‰\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "results = classifier([\n",
    "    \"We are very happy to introduce python and transformers to you.\",\n",
    "    \"So sad you didn't attend the class.\"\n",
    "])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transformers.Trainer`: è®­ç»ƒæ¨¡å‹çš„API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»ä»¥è®­ç»ƒæƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ä¸ºä¾‹\n",
    "\n",
    "# è½½å…¥é¢„è®­ç»ƒæ¨¡å‹\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½½å…¥æ•°æ®é›†\n",
    "# è½½å…¥ä½¿ç”¨çš„tokenizerå¹¶å®šä¹‰é¢„å¤„ç†å‡½æ•°\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "dataset = dataset.map(tokenize_dataset, batched=True) # apply tokenize function to the dataset\n",
    "\n",
    "# ä»æ•°æ®é›†è·å–æ•°æ®å¹¶ä¸ºè®­ç»ƒæä¾›æ‰¹æ¬¡ï¼ˆbatchï¼‰çš„æ•°æ®\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®è®­ç»ƒå‚æ•°\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"path/to/save/folder/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿›è¡Œtrainerçš„åˆå§‹åŒ–\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")  # doctest: +SKIP\n",
    "\n",
    "# ä½¿ç”¨trainerè¿›è¡Œè®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Diffusion\n",
    "\n",
    "> ä¸»è¦ä½¿ç”¨diffuserï¼šhuggingfaceæ¨å‡ºçš„ä½¿ç”¨diffusion modelçš„åº“ã€‚\n",
    "> https://huggingface.co/docs/diffusers/index\n",
    "> WebUIæä¾›äº†æ›´åŠ ä¾¿æ·çš„LoRAæ¥å£ï¼Œä¹Ÿæœ‰æ›´ä¸°å¯Œçš„LoRAç¤¾åŒºå»ºè®¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–åº“\n",
    "%pip install diffusers\n",
    "%pip install safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œå›¾åƒåˆ°å›¾åƒç”Ÿæˆï¼Œå›¾åƒè¡¥å…¨ï¼Œ etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# Text-to-image generation: æ ¹æ®æ–‡æœ¬ç”Ÿæˆå›¾åƒ\n",
    "prompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "image.save(\"generations/t2i.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inpainting: å›¾åƒè¡¥å…¨\n",
    "from diffusers import StableDiffusionXLInpaintPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "init_image = load_image(img_url).convert(\"RGB\")\n",
    "mask_image = load_image(mask_url).convert(\"RGB\")\n",
    "\n",
    "prompt = \"A majestic tiger sitting on a bench\"\n",
    "image = pipe(prompt=prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]\n",
    "image.save(\"generations/inpaint.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-to-image generation: å›¾åƒåˆ°å›¾åƒç”Ÿæˆ\n",
    "from diffusers import StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "# ä½¿ç”¨é€‚é…è¯¥ä»»åŠ¡çš„æ¨¡å‹\n",
    "pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "url = \"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/aa_xl/000000009.png\"\n",
    "\n",
    "init_image = load_image(url).convert(\"RGB\")\n",
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt, image=init_image).images[0]\n",
    "image.save(\"generations/i2i.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·»åŠ Loraè¿›è¡Œé£æ ¼åŒ–ç”Ÿæˆ\n",
    "# https://huggingface.co/spaces/multimodalart/LoraTheExplorer\n",
    "# https://civitai.com\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch\n",
    "\n",
    "# åŠ è½½base model\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ")\n",
    "pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lora: Pixel Art XL\n",
    "pipe.load_lora_weights(\"nerijs/pixel-art-xl\", weight_name=\"pixel-art-xl.safetensors\")\n",
    "prompt = \"pixel art...\"\n",
    "lora_scale= 0.9\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5, cross_attention_kwargs={\"scale\": lora_scale}).images[0]\n",
    "image.save(\"image_lora.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio\n",
    "\n",
    "> ç®€å•çš„demoç•Œé¢åˆ¶ä½œåº“ã€‚\n",
    "> å®˜æ–¹æ–‡æ¡£ï¼šhttps://www.gradio.app/guides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…\n",
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hello world\n",
    "# ä½œä¸ºpythonè„šæœ¬æ—¶ï¼Œä»¥`gradio app.py`çš„å½¢å¼è¿è¡Œã€‚\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return f\"Hello world, {name}!\"\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¤šä¸ªä¸åŒç§ç±»çš„è¾“å…¥\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name, is_morning, temperature):\n",
    "    salutation = \"Good morning\" if is_morning else \"Good evening\"\n",
    "    greeting = f\"{salutation} {name}. It is {temperature} degrees today\"\n",
    "    celsius = (temperature - 32) * 5 / 9\n",
    "    return greeting, round(celsius, 2)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n",
    "    outputs=[\"text\", \"number\"],\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¾“å…¥åŠè¾“å‡ºå›¾åƒçš„æ ·ä¾‹\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def sepia(input_img):\n",
    "    sepia_filter = np.array([\n",
    "        [0.393, 0.769, 0.189], \n",
    "        [0.349, 0.686, 0.168], \n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    sepia_img = input_img.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    return sepia_img\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradio.Blocks() æ¨¡å—åŒ–å®ç°æ›´å¤æ‚çš„gradioé…ç½®\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def flip_text(x):\n",
    "    return x[::-1]\n",
    "\n",
    "def flip_image(x):\n",
    "    return np.fliplr(x)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Flip text or image files using this demo.\")\n",
    "    with gr.Tab(\"Flip Text\"):\n",
    "        text_input = gr.Textbox()\n",
    "        text_output = gr.Textbox()\n",
    "        text_button = gr.Button(\"Flip\")\n",
    "    with gr.Tab(\"Flip Image\"):\n",
    "        with gr.Row():\n",
    "            image_input = gr.Image()\n",
    "            image_output = gr.Image()\n",
    "        image_button = gr.Button(\"Flip\")\n",
    "\n",
    "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
    "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "\n",
    "Have fun with channels of Stable Diffusion gradio demos!\n",
    "- https://huggingface.co/spaces/Manjushri/SDXL-1.0\n",
    "- https://stablediffusion.fr/sdxl\n",
    "- https://huggingface.co/spaces/multimodalart/LoraTheExplorer\n",
    "- ...\n",
    "\n",
    "Example prompts:\n",
    "- breathtaking night street of Tokyo, neon lights. award-winning, professional, highly detailed\n",
    "- anime artwork an empty classroom. anime style, key visual, vibrant, studio anime, highly detailed\n",
    "- concept art of dragon flying over town, clouds. digital artwork, illustrative, painterly, matte painting, highly detailed, cinematic composition\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
